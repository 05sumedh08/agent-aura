{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c560a5b3",
   "metadata": {
    "papermill": {
     "duration": 0.002533,
     "end_time": "2025-12-02T02:17:34.114639",
     "exception": false,
     "start_time": "2025-12-02T02:17:34.112106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# CSIRO Biomass Prediction - Baseline Solution\n",
    "\n",
    "This notebook implements a baseline solution for the CSIRO Biomass Prediction competition.\n",
    "It uses an EfficientNet-B0 model to predict 3 independent biomass components (`Dry_Green_g`, `Dry_Clover_g`, `Dry_Dead_g`) and derives the others (`GDM_g`, `Dry_Total_g`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabd3b84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T02:17:34.120156Z",
     "iopub.status.busy": "2025-12-02T02:17:34.119550Z",
     "iopub.status.idle": "2025-12-02T02:17:46.197121Z",
     "shell.execute_reply": "2025-12-02T02:17:46.196364Z"
    },
    "papermill": {
     "duration": 12.081404,
     "end_time": "2025-12-02T02:17:46.198332",
     "exception": false,
     "start_time": "2025-12-02T02:17:34.116928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cf2e1",
   "metadata": {
    "papermill": {
     "duration": 0.001895,
     "end_time": "2025-12-02T02:17:46.202355",
     "exception": false,
     "start_time": "2025-12-02T02:17:46.200460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe3f5fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T02:17:46.207518Z",
     "iopub.status.busy": "2025-12-02T02:17:46.206852Z",
     "iopub.status.idle": "2025-12-02T02:17:46.215545Z",
     "shell.execute_reply": "2025-12-02T02:17:46.215035Z"
    },
    "papermill": {
     "duration": 0.012369,
     "end_time": "2025-12-02T02:17:46.216564",
     "exception": false,
     "start_time": "2025-12-02T02:17:46.204195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class BiomassDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, is_test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            is_test (bool): If True, csv_file is treated as test.csv (no targets).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        if not is_test:\n",
    "            # Pivot to have one row per image\n",
    "            # We need Dry_Green_g, Dry_Clover_g, Dry_Dead_g\n",
    "            self.data = df.pivot(index='image_path', columns='target_name', values='target').reset_index()\n",
    "            # Ensure columns exist\n",
    "            for col in ['Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g']:\n",
    "                if col not in self.data.columns:\n",
    "                    self.data[col] = 0.0\n",
    "            \n",
    "            # Filter to keep only necessary columns\n",
    "            self.data = self.data[['image_path', 'Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g']]\n",
    "        else:\n",
    "            # For test, we just need unique images\n",
    "            self.data = df[['image_path']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['image_path'])\n",
    "        \n",
    "        # Handle potential path issues (e.g. if csv has 'train/ID...' and root_dir is '.../train')\n",
    "        # The csv seems to have 'train/ID...' and images are in 'train' folder.\n",
    "        # So if root_dir is the base folder, it should work.\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            # Try adjusting path if needed\n",
    "            # If img_path is 's:/.../train/train/ID...'\n",
    "            # Let's assume root_dir is the base directory containing 'train' and 'test' folders\n",
    "            pass\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if not self.is_test:\n",
    "            # Get targets\n",
    "            dry_green = self.data.iloc[idx]['Dry_Green_g']\n",
    "            dry_clover = self.data.iloc[idx]['Dry_Clover_g']\n",
    "            dry_dead = self.data.iloc[idx]['Dry_Dead_g']\n",
    "            \n",
    "            targets = torch.tensor([dry_green, dry_clover, dry_dead], dtype=torch.float32)\n",
    "            return image, targets, self.data.iloc[idx]['image_path']\n",
    "        else:\n",
    "            return image, self.data.iloc[idx]['image_path']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503dea90",
   "metadata": {
    "papermill": {
     "duration": 0.001793,
     "end_time": "2025-12-02T02:17:46.220202",
     "exception": false,
     "start_time": "2025-12-02T02:17:46.218409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c4c5075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T02:17:46.224913Z",
     "iopub.status.busy": "2025-12-02T02:17:46.224454Z",
     "iopub.status.idle": "2025-12-02T02:17:46.229310Z",
     "shell.execute_reply": "2025-12-02T02:17:46.228814Z"
    },
    "papermill": {
     "duration": 0.008292,
     "end_time": "2025-12-02T02:17:46.230304",
     "exception": false,
     "start_time": "2025-12-02T02:17:46.222012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class BiomassModel(nn.Module):\n",
    "    def __init__(self, num_outputs=3):\n",
    "        super(BiomassModel, self).__init__()\n",
    "        # Use EfficientNet B0 as backbone\n",
    "        # weights='DEFAULT' loads the best available weights (ImageNet)\n",
    "        try:\n",
    "            self.backbone = models.efficientnet_b0(weights='DEFAULT')\n",
    "            num_ftrs = self.backbone.classifier[1].in_features\n",
    "            self.backbone.classifier = nn.Identity() # Remove original classifier\n",
    "        except:\n",
    "            # Fallback to ResNet18 if EfficientNet is not available\n",
    "            print(\"EfficientNet not found, using ResNet18\")\n",
    "            self.backbone = models.resnet18(pretrained=True)\n",
    "            num_ftrs = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_outputs),\n",
    "            nn.ReLU() # Ensure non-negative output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00284105",
   "metadata": {
    "papermill": {
     "duration": 0.00188,
     "end_time": "2025-12-02T02:17:46.234061",
     "exception": false,
     "start_time": "2025-12-02T02:17:46.232181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f903136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T02:17:46.238918Z",
     "iopub.status.busy": "2025-12-02T02:17:46.238379Z",
     "iopub.status.idle": "2025-12-02T02:17:46.248650Z",
     "shell.execute_reply": "2025-12-02T02:17:46.247980Z"
    },
    "papermill": {
     "duration": 0.01385,
     "end_time": "2025-12-02T02:17:46.249732",
     "exception": false,
     "start_time": "2025-12-02T02:17:46.235882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(num_epochs=2, batch_size=16, learning_rate=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Data Transforms\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # Dataset\n",
    "    full_dataset = BiomassDataset(\n",
    "        csv_file='/kaggle/input/csiro-biomass/train.csv',\n",
    "        root_dir='/kaggle/input/csiro-biomass',\n",
    "        transform=data_transforms['train'] # Initial transform, will override for val\n",
    "    )\n",
    "\n",
    "    # Split indices\n",
    "    dataset_size = len(full_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Samplers\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "    # For validation, we strictly should use val transforms, but SubsetRandomSampler makes it hard to change transform per sample easily without two datasets.\n",
    "    # For simplicity, we'll use the same dataset but maybe without heavy augmentation if possible.\n",
    "    # Actually, let's create two dataset instances.\n",
    "    \n",
    "    train_dataset = BiomassDataset(\n",
    "        csv_file='/kaggle/input/csiro-biomass/train.csv',\n",
    "        root_dir='/kaggle/input/csiro-biomass',\n",
    "        transform=data_transforms['train']\n",
    "    )\n",
    "    val_dataset = BiomassDataset(\n",
    "        csv_file='/kaggle/input/csiro-biomass/train.csv',\n",
    "        root_dir='/kaggle/input/csiro-biomass',\n",
    "        transform=data_transforms['val']\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler, num_workers=0)\n",
    "\n",
    "    # Model\n",
    "    model = BiomassModel(num_outputs=3).to(device)\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for inputs, targets, _ in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.sampler) # Use sampler length\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "                print(\"Model saved.\")\n",
    "\n",
    "    print(f'Best Val Loss: {best_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505600a0",
   "metadata": {
    "papermill": {
     "duration": 0.00184,
     "end_time": "2025-12-02T02:17:46.253630",
     "exception": false,
     "start_time": "2025-12-02T02:17:46.251790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdae54b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T02:17:46.258762Z",
     "iopub.status.busy": "2025-12-02T02:17:46.258368Z",
     "iopub.status.idle": "2025-12-02T02:17:46.265364Z",
     "shell.execute_reply": "2025-12-02T02:17:46.264879Z"
    },
    "papermill": {
     "duration": 0.01065,
     "end_time": "2025-12-02T02:17:46.266339",
     "exception": false,
     "start_time": "2025-12-02T02:17:46.255689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Dataset\n",
    "    test_dataset = BiomassDataset(\n",
    "        csv_file='/kaggle/input/csiro-biomass/test.csv',\n",
    "        root_dir='/kaggle/input/csiro-biomass',\n",
    "        transform=transform,\n",
    "        is_test=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model\n",
    "    model = BiomassModel(num_outputs=3).to(device)\n",
    "    model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(\"Starting prediction...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, image_paths in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Outputs: [Dry_Green_g, Dry_Clover_g, Dry_Dead_g]\n",
    "            dry_green = outputs[0][0].item()\n",
    "            dry_clover = outputs[0][1].item()\n",
    "            dry_dead = outputs[0][2].item()\n",
    "            \n",
    "            # Derived\n",
    "            gdm = dry_green + dry_clover # Assuming GDM is Green + Clover? Wait, let's check check_data_output_2.txt again.\n",
    "            # H2: GDM_g = Dry_Green_g + Dry_Clover_g. Yes.\n",
    "            \n",
    "            dry_total = gdm + dry_dead\n",
    "            # H1: Dry_Total_g = GDM_g + Dry_Dead_g. Yes.\n",
    "            \n",
    "            # Image ID from path\n",
    "            # image_path is like 'test/ID1001187975.jpg'\n",
    "            # We need ID1001187975\n",
    "            img_path = image_paths[0]\n",
    "            basename = os.path.basename(img_path)\n",
    "            image_id = os.path.splitext(basename)[0]\n",
    "            \n",
    "            # Append results for each target type\n",
    "            results.append({'sample_id': f\"{image_id}__Dry_Green_g\", 'target': dry_green})\n",
    "            results.append({'sample_id': f\"{image_id}__Dry_Clover_g\", 'target': dry_clover})\n",
    "            results.append({'sample_id': f\"{image_id}__Dry_Dead_g\", 'target': dry_dead})\n",
    "            results.append({'sample_id': f\"{image_id}__GDM_g\", 'target': gdm})\n",
    "            results.append({'sample_id': f\"{image_id}__Dry_Total_g\", 'target': dry_total})\n",
    "\n",
    "    # Create DataFrame\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission saved to submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956d582",
   "metadata": {
    "papermill": {
     "duration": 0.001862,
     "end_time": "2025-12-02T02:17:46.270060",
     "exception": false,
     "start_time": "2025-12-02T02:17:46.268198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa07fc19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T02:17:46.274538Z",
     "iopub.status.busy": "2025-12-02T02:17:46.274347Z",
     "iopub.status.idle": "2025-12-02T02:19:33.045167Z",
     "shell.execute_reply": "2025-12-02T02:19:33.044189Z"
    },
    "papermill": {
     "duration": 106.774567,
     "end_time": "2025-12-02T02:19:33.046431",
     "exception": false,
     "start_time": "2025-12-02T02:17:46.271864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 182MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n",
      "train Loss: 628.3000\n",
      "val Loss: 507.5863\n",
      "Model saved.\n",
      "Epoch 2/5\n",
      "----------\n",
      "train Loss: 599.4013\n",
      "val Loss: 468.7160\n",
      "Model saved.\n",
      "Epoch 3/5\n",
      "----------\n",
      "train Loss: 544.3199\n",
      "val Loss: 384.9710\n",
      "Model saved.\n",
      "Epoch 4/5\n",
      "----------\n",
      "train Loss: 460.0891\n",
      "val Loss: 323.2371\n",
      "Model saved.\n",
      "Epoch 5/5\n",
      "----------\n",
      "train Loss: 379.9856\n",
      "val Loss: 303.9390\n",
      "Model saved.\n",
      "Best Val Loss: 303.9390\n",
      "Using device: cuda\n",
      "Starting prediction...\n",
      "Submission saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "# Reduced epochs for demonstration, increase for better results\n",
    "train_model(num_epochs=5, batch_size=32, learning_rate=1e-4)\n",
    "\n",
    "# Generate submission\n",
    "predict()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 125.066955,
   "end_time": "2025-12-02T02:19:35.294989",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-02T02:17:30.228034",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
